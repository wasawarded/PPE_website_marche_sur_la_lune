   #alternate Edit this page Wikipedia (en) Wikipedia Atom feed

   Jump to content

   [ ] Main menu
   Main menu
   (BUTTON) move to sidebar (BUTTON) hide
   Navigation
     * Main page
     * Contents
     * Current events
     * Random article
     * About Wikipedia
     * Contact us

   Contribute
     * Help
     * Learn to edit
     * Community portal
     * Recent changes
     * Upload file

   Wikipedia The Free Encyclopedia
   Search
   ____________________
   (BUTTON) Search

   [ ] Appearance

     * Donate
     * Create account
     * Log in

   [ ] Personal tools
     * Donate
     * Create account
     * Log in

   Pages for logged out editors learn more
     * Contributions
     * Talk

Contents

   (BUTTON) move to sidebar (BUTTON) hide
     * (Top)
     * 1 History
     * 2 Setting
       (BUTTON) Toggle Setting subsection
          + 2.1 Learning from entailment
          + 2.2 Learning from interpretations
     * 3 Approaches to ILP
       (BUTTON) Toggle Approaches to ILP subsection
          + 3.1 Bottom-up search
               o 3.1.1 Least general generalisation
               o 3.1.2 Inverse resolution
          + 3.2 Top-down search
          + 3.3 Metainterpretive learning
          + 3.4 Evolutionary learning
     * 4 List of implementations
     * 5 Probabilistic inductive logic programming
       (BUTTON) Toggle Probabilistic inductive logic programming
       subsection
          + 5.1 Parameter Learning
          + 5.2 Structure Learning
     * 6 See also
     * 7 References
     * 8 Further reading

   [ ] Toggle the table of contents

Inductive logic programming

   [ ] 13 languages
     * العربية
     * Català
     * Čeština
     * Deutsch
     * Español
     * فارسی
     * Français
     * Italiano
     * Português
     * Русский
     * Српски / srpski
     * ไทย
     * Türkçe

   Edit links

     * Article
     * Talk

   [ ] English

     * Read
     * Edit
     * View history

   [ ] Tools
   Tools
   (BUTTON) move to sidebar (BUTTON) hide
   Actions
     * Read
     * Edit
     * View history

   General
     * What links here
     * Related changes
     * Upload file
     * Special pages
     * Permanent link
     * Page information
     * Cite this page
     * Get shortened URL
     * Download QR code

   Print/export
     * Download as PDF
     * Printable version

   In other projects
     * Wikimedia Commons
     * Wikidata item

   Appearance
   (BUTTON) move to sidebar (BUTTON) hide
   From Wikipedia, the free encyclopedia
   [220px-ILP_family2.png] A photo of Family sample for Inductive Logic
   Programming article

   Inductive logic programming (ILP) is a subfield of symbolic artificial
   intelligence which uses logic programming as a uniform representation
   for examples, background knowledge and hypotheses. The term "inductive"
   here refers to philosophical (i.e. suggesting a theory to explain
   observed facts) rather than mathematical (i.e. proving a property for
   all members of a well-ordered set) induction. Given an encoding of the
   known background knowledge and a set of examples represented as a
   logical database of facts, an ILP system will derive a hypothesised
   logic program which entails all the positive and none of the negative
   examples.
     * Schema: positive examples + negative examples + background
       knowledge ⇒ hypothesis.

   Inductive logic programming is particularly useful in bioinformatics
   and natural language processing.

History

   [edit]

   Building on earlier work on Inductive inference, Gordon Plotkin was the
   first to formalise induction in a clausal setting around 1970, adopting
   an approach of generalising from examples.^[1]^[2] In 1981, Ehud
   Shapiro introduced several ideas that would shape the field in his new
   approach of model inference, an algorithm employing refinement and
   backtracing to search for a complete axiomatisation of given
   examples.^[1]^[3] His first implementation was the Model Inference
   System in 1981:^[4]^[5] a Prolog program that inductively inferred Horn
   clause logic programs from positive and negative examples.^[1] The term
   Inductive Logic Programming was first introduced in a paper by Stephen
   Muggleton in 1990, defined as the intersection of machine learning and
   logic programming.^[1] Muggleton and Wray Buntine introduced predicate
   invention and inverse resolution in 1988.^[1]^[6]

   Several inductive logic programming systems that proved influential
   appeared in the early 1990s. FOIL, introduced by Ross Quinlan in
   1990^[7] was based on upgrading propositional learning algorithms AQ
   and ID3.^[8] Golem, introduced by Muggleton and Feng in 1990, went back
   to a restricted form of Plotkin's least generalisation
   algorithm.^[8]^[9] The Progol system, introduced by Muggleton in 1995,
   first implemented inverse entailment, and inspired many later
   systems.^[8]^[10]^[11] Aleph, a descendant of Progol introduced by
   Ashwin Srinivasan in 2001, is still one of the most widely used systems
   as of 2022^[update].^[10]

   At around the same time, the first practical applications emerged,
   particularly in bioinformatics, where by 2000 inductive logic
   programming had been successfully applied to drug design,
   carcinogenicity and mutagenicity prediction, and elucidation of the
   structure and function of proteins.^[12] Unlike the focus on automatic
   programming inherent in the early work, these fields used inductive
   logic programming techniques from a viewpoint of relational data
   mining. The success of those initial applications and the lack of
   progress in recovering larger traditional logic programs shaped the
   focus of the field.^[13]

   Recently, classical tasks from automated programming have moved back
   into focus, as the introduction of meta-interpretative learning makes
   predicate invention and learning recursive programs more feasible. This
   technique was pioneered with the Metagol system introduced by
   Muggleton, Dianhuan Lin, Niels Pahlavi and Alireza Tamaddoni-Nezhad in
   2014.^[14] This allows ILP systems to work with fewer examples, and
   brought successes in learning string transformation programs, answer
   set grammars and general algorithms.^[15]

Setting

   [edit]

   Inductive logic programming has adopted several different learning
   settings, the most common of which are learning from entailment and
   learning from interpretations.^[16] In both cases, the input is
   provided in the form of background knowledge B, a logical theory
   (commonly in the form of clauses used in logic programming), as well as
   positive and negative examples, denoted
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{+}}</annotation> </semantics> :MATH]
   {\textstyle E^{+}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>−</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{-}}</annotation> </semantics> :MATH]
   {\textstyle E^{-}} respectively. The output is given as a hypothesis H,
   itself a logical theory that typically consists of one or more clauses.

   The two settings differ in the format of examples presented.

Learning from entailment

   [edit]

   As of 2022^[update], learning from entailment is by far the most
   popular setting for inductive logic programming.^[16] In this setting,
   the positive and negative examples are given as finite sets
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{+}}</annotation> </semantics> :MATH]
   {\textstyle E^{+}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>−</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{-}}</annotation> </semantics> :MATH]
   {\textstyle E^{-}} of positive and negated ground literals,
   respectively. A correct hypothesis H is a set of clauses satisfying the
   following requirements, where the turnstile symbol
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mo>⊨</mo> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle \models
   }</annotation> </semantics> :MATH]
   {\displaystyle \models } stands for logical entailment:^[16]^[17]^[18]
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mrow class="MJX-TeXAtom-ORD">
   <mtable columnalign="left left left left" rowspacing="4pt"
   columnspacing="1em"> <mtr> <mtd> <mrow class="MJX-TeXAtom-ORD">
   <mtext>Completeness:</mtext> </mrow> </mtd> <mtd> <mi>B</mi> <mo>∪</mo>
   <mi>H</mi> </mtd> <mtd> <mo>⊨</mo> </mtd> <mtd> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup> </mtd> </mtr> <mtr>
   <mtd> <mrow class="MJX-TeXAtom-ORD"> <mtext>Consistency: </mtext>
   </mrow> </mtd> <mtd> <mi>B</mi> <mo>∪</mo> <mi>H</mi> <mo>∪</mo> <msup>
   <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>−</mo> </mrow> </msup>
   </mtd> <mtd> <mo>⊭</mo> </mtd> <mtd> <mrow class="MJX-TeXAtom-ORD">
   <mrow class="MJX-TeXAtom-ORD"> <mtext class="MJX-tex-mathit"
   mathvariant="italic">false</mtext> </mrow> </mrow> </mtd> </mtr>
   </mtable> </mrow> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle
   {\begin{array}{llll}{\text{Completeness:}}&B\cup H&\models
   &E^{+}\\{\text{Consistency: }}&B\cup H\cup E^{-}&\not \models &{\textit
   {false}}\end{array}}}</annotation> </semantics> :MATH]
   {\displaystyle {\begin{array}{llll}{\text{Completeness:}}&B\cup
   H&\models &E^{+}\\{\text{Consistency: }}&B\cup H\cup E^{-}&\not \models
   &{\textit {false}}\end{array}}} Completeness requires any generated
   hypothesis h to explain all positive examples
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{+}}</annotation> </semantics> :MATH]
   {\textstyle E^{+}} , and consistency forbids generation of any
   hypothesis h that is inconsistent with the negative examples
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>−</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{-}}</annotation> </semantics> :MATH]
   {\textstyle E^{-}} , both given the background knowledge B.

   In Muggleton's setting of concept learning,^[19] "completeness" is
   referred to as "sufficiency", and "consistency" as "strong
   consistency". Two further conditions are added: "Necessity", which
   postulates that B does not entail
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   E^{+}}</annotation> </semantics> :MATH]
   {\textstyle E^{+}} , does not impose a restriction on h, but forbids
   any generation of a hypothesis as long as the positive facts are
   explainable without it. . "Weak consistency", which states that no
   contradiction can be derived from
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>B</mi> <mo>∧</mo> <mi>H</mi>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   B\land H}</annotation> </semantics> :MATH]
   {\textstyle B\land H} , forbids generation of any hypothesis h that
   contradicts the background knowledge B. Weak consistency is implied by
   strong consistency; if no negative examples are given, both
   requirements coincide. Weak consistency is particularly important in
   the case of noisy data, where completeness and strong consistency
   cannot be guaranteed.^[19]

Learning from interpretations

   [edit]

   In learning from interpretations, the positive and negative examples
   are given as a set of complete or partial Herbrand structures, each of
   which are themselves a finite set of ground literals. Such a structure
   e is said to be a model of the set of clauses
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>B</mi> <mo>∪</mo> <mi>H</mi>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   B\cup H}</annotation> </semantics> :MATH]
   {\textstyle B\cup H} if for any substitution
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>θ</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle \theta
   }</annotation> </semantics> :MATH]
   {\textstyle \theta } and any clause
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mrow class="MJX-TeXAtom-ORD">
   <mi mathvariant="normal">h</mi> <mi mathvariant="normal">e</mi> <mi
   mathvariant="normal">a</mi> <mi mathvariant="normal">d</mi> </mrow> <mo
   stretchy="false">←</mo> <mrow class="MJX-TeXAtom-ORD"> <mi
   mathvariant="normal">b</mi> <mi mathvariant="normal">o</mi> <mi
   mathvariant="normal">d</mi> <mi mathvariant="normal">y</mi> </mrow>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   \mathrm {head} \leftarrow \mathrm {body} }</annotation> </semantics>
   :MATH]
   {\textstyle \mathrm {head} \leftarrow \mathrm {body} } in
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>B</mi> <mo>∪</mo> <mi>H</mi>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   B\cup H}</annotation> </semantics> :MATH]
   {\textstyle B\cup H} such that
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mrow class="MJX-TeXAtom-ORD">
   <mi mathvariant="normal">b</mi> <mi mathvariant="normal">o</mi> <mi
   mathvariant="normal">d</mi> <mi mathvariant="normal">y</mi> </mrow>
   <mi>θ</mi> <mo>⊆</mo> <mi>e</mi> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\textstyle \mathrm {body} \theta
   \subseteq e}</annotation> </semantics> :MATH]
   {\textstyle \mathrm {body} \theta \subseteq e} ,
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mrow class="MJX-TeXAtom-ORD"> <mi
   mathvariant="normal">h</mi> <mi mathvariant="normal">e</mi> <mi
   mathvariant="normal">a</mi> <mi mathvariant="normal">d</mi> </mrow>
   <mi>θ</mi> <mo>⊆</mo> <mi>e</mi> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle \mathrm {head} \theta
   \subseteq e}</annotation> </semantics> :MATH]
   {\displaystyle \mathrm {head} \theta \subseteq e} also holds. The goal
   is then to output a hypothesis that is complete, meaning every positive
   example is a model of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>B</mi> <mo>∪</mo> <mi>H</mi>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   B\cup H}</annotation> </semantics> :MATH]
   {\textstyle B\cup H} , and consistent, meaning that no negative example
   is a model of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>B</mi> <mo>∪</mo> <mi>H</mi>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   B\cup H}</annotation> </semantics> :MATH]
   {\textstyle B\cup H} .^[16]

Approaches to ILP

   [edit]

   An inductive logic programming system is a program that takes as an
   input logic theories
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>B</mi> <mo>,</mo> <msup>
   <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup>
   <mo>,</mo> <msup> <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>−</mo>
   </mrow> </msup> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle B,E^{+},E^{-}}</annotation>
   </semantics> :MATH]
   {\displaystyle B,E^{+},E^{-}} and outputs a correct hypothesis H with
   respect to theories
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>B</mi> <mo>,</mo> <msup>
   <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup>
   <mo>,</mo> <msup> <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>−</mo>
   </mrow> </msup> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle B,E^{+},E^{-}}</annotation>
   </semantics> :MATH]
   {\displaystyle B,E^{+},E^{-}} . A system is complete if and only if for
   any input logic theories
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>B</mi> <mo>,</mo> <msup>
   <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup>
   <mo>,</mo> <msup> <mi>E</mi> <mrow class="MJX-TeXAtom-ORD"> <mo>−</mo>
   </mrow> </msup> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle B,E^{+},E^{-}}</annotation>
   </semantics> :MATH]
   {\displaystyle B,E^{+},E^{-}} any correct hypothesis H with respect to
   these input theories can be found with its hypothesis search procedure.
   Inductive logic programming systems can be roughly divided into two
   classes, search-based and meta-interpretative systems.

   Search-based systems exploit that the space of possible clauses forms a
   complete lattice under the subsumption relation, where one clause
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} subsumes another clause
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} if there is a substitution
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>θ</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle \theta
   }</annotation> </semantics> :MATH]
   {\textstyle \theta } such that
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> <mi>θ</mi>
   </mstyle> </mrow> <annotation encoding="application/x-tex">{\textstyle
   C_{1}\theta }</annotation> </semantics> :MATH]
   {\textstyle C_{1}\theta } , the result of applying
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>θ</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle \theta
   }</annotation> </semantics> :MATH]
   {\textstyle \theta } to
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} , is a subset of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} . This lattice can be traversed either bottom-up or
   top-down.

Bottom-up search

   [edit]

   Bottom-up methods to search the subsumption lattice have been
   investigated since Plotkin's first work on formalising induction in
   clausal logic in 1970.^[1]^[20] Techniques used include least general
   generalisation, based on anti-unification, and inverse resolution,
   based on inverting the resolution inference rule.

Least general generalisation

   [edit]

   A least general generalisation algorithm takes as input two clauses
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} and outputs the least general generalisation of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} , that is, a clause
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>C</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle C}</annotation>
   </semantics> :MATH]
   {\textstyle C} that subsumes
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} , and that is subsumed by every other clause that
   subsumes
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} . The least general generalisation can be computed
   by first computing all selections from
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>C</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle C}</annotation>
   </semantics> :MATH]
   {\textstyle C} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>D</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle D}</annotation>
   </semantics> :MATH]
   {\textstyle D} , which are pairs of literals
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mo stretchy="false">(</mo>
   <mi>L</mi> <mo>,</mo> <mi>M</mi> <mo stretchy="false">)</mo> <mo>∈</mo>
   <mo stretchy="false">(</mo> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> <mo>,</mo> <msub>
   <mi>C</mi> <mrow class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub>
   <mo stretchy="false">)</mo> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle (L,M)\in
   (C_{1},C_{2})}</annotation> </semantics> :MATH]
   {\displaystyle (L,M)\in (C_{1},C_{2})} sharing the same predicate
   symbol and negated/unnegated status. Then, the least general
   generalisation is obtained as the disjunction of the least general
   generalisations of the individual selections, which can be obtained by
   first-order syntactical anti-unification.^[21]

   To account for background knowledge, inductive logic programming
   systems employ relative least general generalisations, which are
   defined in terms of subsumption relative to a background theory. In
   general, such relative least general generalisations are not guaranteed
   to exist; however, if the background theory B is a finite set of ground
   literals, then the negation of B is itself a clause. In this case, a
   relative least general generalisation can be computed by disjoining the
   negation of B with both
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} and then computing their least general
   generalisation as before.^[22]

   Relative least general generalisations are the foundation of the
   bottom-up system Golem.^[8]^[9]

Inverse resolution

   [edit]

   Inverse resolution is an inductive reasoning technique that involves
   inverting the resolution operator.

   Inverse resolution takes information about the resolvent of a
   resolution step to compute possible resolving clauses. Two types of
   inverse resolution operator are in use in inductive logic programming:
   V-operators and W-operators. A V-operator takes clauses
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>R</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle R}</annotation>
   </semantics> :MATH]
   {\textstyle R} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} as input and returns a clause
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} such that
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>R</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle R}</annotation>
   </semantics> :MATH]
   {\textstyle R} is the resolvent of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} . A W-operator takes two clauses
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>R</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   R_{1}}</annotation> </semantics> :MATH]
   {\textstyle R_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>R</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   R_{2}}</annotation> </semantics> :MATH]
   {\textstyle R_{2}} and returns thre clauses
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} ,
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>3</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{3}}</annotation> </semantics> :MATH]
   {\textstyle C_{3}} such that
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>R</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   R_{1}}</annotation> </semantics> :MATH]
   {\textstyle R_{1}} is the resolvent of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>1</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{1}}</annotation> </semantics> :MATH]
   {\textstyle C_{1}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>R</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   R_{2}}</annotation> </semantics> :MATH]
   {\textstyle R_{2}} is the resolvent of
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>2</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{2}}</annotation> </semantics> :MATH]
   {\textstyle C_{2}} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <msub> <mi>C</mi> <mrow
   class="MJX-TeXAtom-ORD"> <mn>3</mn> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle
   C_{3}}</annotation> </semantics> :MATH]
   {\textstyle C_{3}} .^[23]

   Inverse resolution was first introduced by Stephen Muggleton and Wray
   Buntine in 1988 for use in the inductive logic programming system
   Cigol.^[6] By 1993, this spawned a surge of research into inverse
   resolution operators and their properties.^[23]

Top-down search

   [edit]

   The ILP systems Progol,^[11] Hail ^[24] and Imparo ^[25] find a
   hypothesis H using the principle of the inverse entailment^[11] for
   theories B, E, H:
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>B</mi> <mo>∧</mo> <mi>H</mi>
   <mo>⊨</mo> <mi>E</mi> <mspace width="thickmathspace" /> <mo
   stretchy="false">⟺</mo> <mspace width="thickmathspace" /> <mi>B</mi>
   <mo>∧</mo> <mi mathvariant="normal">¬</mi> <mi>E</mi> <mo>⊨</mo> <mi
   mathvariant="normal">¬</mi> <mi>H</mi> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle B\land H\models E\iff
   B\land \neg E\models \neg H}</annotation> </semantics> :MATH]
   {\displaystyle B\land H\models E\iff B\land \neg E\models \neg H} .
   First they construct an intermediate theory F called a bridge theory
   satisfying the conditions
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>B</mi> <mo>∧</mo> <mi
   mathvariant="normal">¬</mi> <mi>E</mi> <mo>⊨</mo> <mi>F</mi> </mstyle>
   </mrow> <annotation encoding="application/x-tex">{\displaystyle B\land
   \neg E\models F}</annotation> </semantics> :MATH]
   {\displaystyle B\land \neg E\models F} and
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>F</mi> <mo>⊨</mo> <mi
   mathvariant="normal">¬</mi> <mi>H</mi> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle F\models \neg
   H}</annotation> </semantics> :MATH]
   {\displaystyle F\models \neg H} . Then as
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>H</mi> <mo>⊨</mo> <mi
   mathvariant="normal">¬</mi> <mi>F</mi> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle H\models \neg
   F}</annotation> </semantics> :MATH]
   {\displaystyle H\models \neg F} , they generalize the negation of the
   bridge theory F with anti-entailment.^[26] However, the operation of
   anti-entailment is computationally more expensive since it is highly
   nondeterministic. Therefore, an alternative hypothesis search can be
   conducted using the inverse subsumption (anti-subsumption) operation
   instead, which is less non-deterministic than anti-entailment.

   Questions of completeness of a hypothesis search procedure of specific
   inductive logic programming system arise. For example, the Progol
   hypothesis search procedure based on the inverse entailment inference
   rule is not complete by Yamamoto's example.^[27] On the other hand,
   Imparo is complete by both anti-entailment procedure ^[28] and its
   extended inverse subsumption ^[29] procedure.

Metainterpretive learning

   [edit]

   Rather than explicitly searching the hypothesis graph, metainterpretive
   or meta-level systems encode the inductive logic programming program as
   a meta-level logic program which is then solved to obtain an optimal
   hypothesis. Formalisms used to express the problem specification
   include Prolog and answer set programming, with existing Prolog systems
   and answer set solvers used for solving the constraints.^[30]

   And example of a Prolog-based system is Metagol, which is based on a
   meta-interpreter in Prolog, while ASPAL and ILASP are based on an
   encoding of the inductive logic programming problem in answer set
   programming.^[30]

Evolutionary learning

   [edit]

   Evolutionary algorithms in ILP use a population-based approach to
   evolve hypotheses, refining them through selection, crossover, and
   mutation. Methods like EvoLearner have been shown to outperform
   traditional approaches on structured machine learning benchmarks. ^[31]

List of implementations

   [edit]
     * 1BC and 1BC2: first-order naive Bayesian classifiers:
     * ACE (A Combined Engine)
     * Aleph
     * Atom Archived 2014-03-26 at the Wayback Machine
     * Claudien^[permanent dead link‍]
     * DL-Learner Archived 2019-08-15 at the Wayback Machine
     * DMax
     * FastLAS (Fast Learning from Answer Sets)
     * FOIL (First Order Inductive Learner)
     * Golem
     * ILASP (Inductive Learning of Answer Set Programs)
     * Imparo^[28]
     * Inthelex (INcremental THEory Learner from EXamples) Archived
       2011-11-28 at the Wayback Machine
     * Lime
     * Metagol
     * Mio
     * MIS (Model Inference System) by Ehud Shapiro
     * Ontolearn
     * Popper
     * PROGOL
     * RSD
     * Warmr (now included in ACE)
     * ProGolem ^[32]^[33]

Probabilistic inductive logic programming

   [edit]

   Probabilistic inductive logic programming adapts the setting of
   inductive logic programming to learning probabilistic logic programs.
   It can be considered as a form of statistical relational learning
   within the formalism of probabilistic logic programming.^[34]^[35]

   Given
    1. background knowledge as a probabilistic logic program B, and
    2. a set of positive and negative examples
       [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
       displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
       class="MJX-TeXAtom-ORD"> <mo>+</mo> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\textstyle
       E^{+}}</annotation> </semantics> :MATH]
       {\textstyle E^{+}} and
       [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
       displaystyle="false" scriptlevel="0"> <msup> <mi>E</mi> <mrow
       class="MJX-TeXAtom-ORD"> <mo>−</mo> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\textstyle
       E^{-}}</annotation> </semantics> :MATH]
       {\textstyle E^{-}}

   the goal of probabilistic inductive logic programming is to find a
   probabilistic logic program
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mi>H</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\textstyle H}</annotation>
   </semantics> :MATH]
   {\textstyle H} such that the probability of positive examples according
   to
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
   displaystyle="false" scriptlevel="0"> <mrow class="MJX-TeXAtom-ORD">
   <mi>H</mi> <mo>∪</mo> <mi>B</mi> </mrow> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\textstyle {H\cup B}}</annotation>
   </semantics> :MATH]
   {\textstyle {H\cup B}} is maximized and the probability of negative
   examples is minimized.^[35]

   This problem has two variants: parameter learning and structure
   learning. In the former, one is given the structure (the clauses) of H
   and the goal is to infer the probabilities annotations of the given
   clauses, while in the latter the goal is to infer both the structure
   and the probability parameters of H. Just as in classical inductive
   logic programming, the examples can be given as examples or as
   (partial) interpretations.^[35]

Parameter Learning

   [edit]

   Parameter learning for languages following the distribution semantics
   has been performed by using an expectation-maximisation algorithm or by
   gradient descent. An expectation-maximisation algorithm consists of a
   cycle in which the steps of expectation and maximization are repeatedly
   performed. In the expectation step, the distribution of the hidden
   variables is computed according to the current values of the
   probability parameters, while in the maximisation step, the new values
   of the parameters are computed. Gradient descent methods compute the
   gradient of the target function and iteratively modify the parameters
   moving in the direction of the gradient.^[35]

Structure Learning

   [edit]

   Structure learning was pioneered by Daphne Koller and Avi Pfeffer in
   1997,^[36] where the authors learn the structure of first-order rules
   with associated probabilistic uncertainty parameters. Their approach
   involves generating the underlying graphical model in a preliminary
   step and then applying expectation-maximisation.^[35]

   In 2008, De Raedt et al. presented an algorithm for performing theory
   compression on ProbLog programs, where theory compression refers to a
   process of removing as many clauses as possible from the theory in
   order to maximize the probability of a given set of positive and
   negative examples. No new clause can be added to the theory.^[35]^[37]

   In the same year, Meert, W. et al. introduced a method for learning
   parameters and structure of ground probabilistic logic programs by
   considering the Bayesian networks equivalent to them and applying
   techniques for learning Bayesian networks.^[38]^[35]

   ProbFOIL, introduced by De Raedt and Ingo Thon in 2010, combined the
   inductive logic programming system FOIL with ProbLog. Logical rules are
   learned from probabilistic data in the sense that both the examples
   themselves and their classifications can be probabilistic. The set of
   rules has to allow one to predict the probability of the examples from
   their description. In this setting, the parameters (the probability
   values) are fixed and the structure has to be learned.^[39]^[35]

   In 2011, Elena Bellodi and Fabrizio Riguzzi introduced SLIPCASE, which
   performs a beam search among probabilistic logic programs by
   iteratively refining probabilistic theories and optimizing the
   parameters of each theory using expectation-maximisation.^[40] Its
   extension SLIPCOVER, proposed in 2014, uses bottom clauses generated as
   in Progol to guide the refinement process, thus reducing the number of
   revisions and exploring the search space more effectively. Moreover,
   SLIPCOVER separates the search for promising clauses from that of the
   theory: the space of clauses is explored with a beam search, while the
   space of theories is searched greedily.^[41]^[35]

See also

   [edit]
     * Commonsense reasoning
     * Formal concept analysis
     * Inductive reasoning
     * Inductive programming
     * Inductive probability
     * Statistical relational learning
     * Version space learning

References

   [edit]
    1. ^ ^a ^b ^c ^d ^e ^f Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de
       (1997). Foundations of inductive logic programming. Lecture notes
       in computer science Lecture notes in artificial intelligence.
       Berlin Heidelberg: Springer. pp. 174–177. ISBN 978-3-540-62927-6.
    2. ^ Plotkin, G.D. (1970). Automatic Methods of Inductive Inference
       (PDF) (PhD). University of Edinburgh. hdl:1842/6656.
    3. ^ Shapiro, Ehud Y. (1981). Inductive inference of theories from
       facts (PDF) (Technical report). Department of Computer Science,
       Yale University. 192. Reprinted in

   Lassez, J.-L.; Plotkin, G., eds. (1991). Computational logic : essays
   in honor of Alan Robinson. MIT Press. pp. 199–254.
   ISBN 978-0-262-12156-9.

     ^ Shapiro, Ehud Y. (1981). "The model inference system" (PDF).
   Proceedings of the 7th international joint conference on Artificial
   intelligence. Vol. 2. Morgan Kaufmann. p. 1064.

     ^ Shapiro, Ehud Y. (1983). Algorithmic program debugging. MIT Press.
   ISBN 0-262-19218-7.

     ^ ^a ^b Muggleton, S.H.; Buntine, W. (1988). "Machine invention of
   first-order predicate by inverting resolution". Proceedings of the 5th
   International Conference on Machine Learning. pp. 339–352.
   doi:10.1016/B978-0-934613-64-4.50040-2. ISBN 978-0-934613-64-4.

     ^ Quinlan, J. R. (August 1990). "Learning logical definitions from
   relations". Machine Learning. 5 (3): 239–266. doi:10.1007/bf00117105.
   ISSN 0885-6125.

     ^ ^a ^b ^c ^d Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997).
   Foundations of inductive logic programming. Lecture notes in computer
   science Lecture notes in artificial intelligence. Berlin Heidelberg:
   Springer. pp. 354–358. ISBN 978-3-540-62927-6.

     ^ ^a ^b Muggleton, Stephen H.; Feng, Cao (1990). Arikawa, Setsuo;
   Goto, Shigeki; Ohsuga, Setsuo; Yokomori, Takashi (eds.). "Efficient
   Induction of Logic Programs". Algorithmic Learning Theory, First
   International Workshop, ALT '90, Tokyo, Japan, October 8–10, 1990,
   Proceedings. Springer/Ohmsha: 368–381.

     ^ ^a ^b Cropper, Andrew; Dumančić, Sebastijan (2022-06-15).
   "Inductive Logic Programming At 30: A New Introduction". Journal of
   Artificial Intelligence Research. 74: 808. arXiv:2008.07912.
   doi:10.1613/jair.1.13507. ISSN 1076-9757.

     ^ ^a ^b ^c Muggleton, S.H. (1995). "Inverting entailment and Progol".
   New Generation Computing. 13 (3–4): 245–286. CiteSeerX 10.1.1.31.1630.
   doi:10.1007/bf03037227. S2CID 12643399.

     ^ Džeroski, Sašo (2001), Džeroski, Sašo; Lavrač, Nada (eds.),
   "Relational Data Mining Applications: An Overview", Relational Data
   Mining, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 339–364,
   doi:10.1007/978-3-662-04599-2_14, ISBN 978-3-642-07604-6, retrieved
   2023-11-27

     ^ De Raedt, Luc (2008), Logical and Relational Learning, Cognitive
   Technologies, Berlin, Heidelberg: Springer, p. 14,
   Bibcode:2008lrl..book.....D, doi:10.1007/978-3-540-68856-3,
   ISBN 978-3-540-20040-6

     ^ Muggleton, Stephen H.; Lin, Dianhuan; Pahlavi, Niels;
   Tamaddoni-Nezhad, Alireza (2013-05-01). "Meta-interpretive learning:
   application to grammatical inference". Machine Learning. 94 (1): 25–49.
   doi:10.1007/s10994-013-5358-3. ISSN 0885-6125. S2CID 254738603.

     ^ Cropper, Andrew; Dumančić, Sebastijan; Evans, Richard; Muggleton,
   Stephen (2022). "Inductive logic programming at 30". Machine Learning.
   111 (1): 147–172. doi:10.1007/s10994-021-06089-1. ISSN 0885-6125.

     ^ ^a ^b ^c ^d Cropper, Andrew; Dumančić, Sebastijan (2022-06-15).
   "Inductive Logic Programming At 30: A New Introduction". Journal of
   Artificial Intelligence Research. 74: 779–782. arXiv:2008.07912.
   doi:10.1613/jair.1.13507. ISSN 1076-9757.

     ^ Džeroski, Sašo (1996). "Inductive Logic Programming and Knowledge
   Discovery in Databases" (PDF). In Fayyad, U.M.; Piatetsky-Shapiro, G.;
   Smith, P.; Uthurusamy, R. (eds.). Advances in Knowledge Discovery and
   Data Mining. MIT Press. pp. 117–152 See §5.2.4. Archived from the
   original (PDF) on 2021-09-27. Retrieved 2021-09-27.

     ^ De Raedt, Luc (1997). "Logical settings for concept-learning".
   Artificial Intelligence. 95 (1): 187–201.
   doi:10.1016/S0004-3702(97)00041-6.

     ^ ^a ^b Muggleton, Stephen (1999). "Inductive Logic Programming:
   Issues, Results and the Challenge of Learning Language in Logic".
   Artificial Intelligence. 114 (1–2): 283–296.
   doi:10.1016/s0004-3702(99)00067-3.; here: Sect.2.1

     ^ Plotkin, G.D. (1970). Automatic Methods of Inductive Inference
   (PDF) (PhD). University of Edinburgh. hdl:1842/6656.

     ^ Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997). Foundations of
   inductive logic programming. Lecture notes in computer science Lecture
   notes in artificial intelligence. Berlin Heidelberg: Springer. p. 255.
   ISBN 978-3-540-62927-6.

     ^ Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997). Foundations of
   inductive logic programming. Lecture notes in computer science Lecture
   notes in artificial intelligence. Berlin Heidelberg: Springer. p. 286.
   ISBN 978-3-540-62927-6.

     ^ ^a ^b Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997).
   Foundations of inductive logic programming. Lecture notes in computer
   science Lecture notes in artificial intelligence. Berlin Heidelberg:
   Springer. p. 197. ISBN 978-3-540-62927-6.

     ^ Ray, O.; Broda, K.; Russo, A.M. (2003). "Hybrid abductive inductive
   learning". Proceedings of the 13th international conference on
   inductive logic programming. LNCS. Vol. 2835. Springer. pp. 311–328.
   CiteSeerX 10.1.1.212.6602. doi:10.1007/978-3-540-39917-9_21.
   ISBN 978-3-540-39917-9.

     ^ Kimber, T.; Broda, K.; Russo, A. (2009). "Induction on failure:
   learning connected Horn theories". Proceedings of the 10th
   international conference on logic programing and nonmonotonic
   reasoning. LNCS. Vol. 575. Springer. pp. 169–181.
   doi:10.1007/978-3-642-04238-6_16. ISBN 978-3-642-04238-6.

     ^ Yamamoto, Yoshitaka; Inoue, Katsumi; Iwanuma, Koji (2012). "Inverse
   subsumption for complete explanatory induction" (PDF). Machine
   Learning. 86: 115–139. doi:10.1007/s10994-011-5250-y. S2CID 11347607.

     ^ Yamamoto, Akihiro (1997). "Which hypotheses can be found with
   inverse entailment?". International Conference on Inductive Logic
   Programming. Lecture Notes in Computer Science. Vol. 1297. Springer.
   pp. 296–308. CiteSeerX 10.1.1.54.2975. doi:10.1007/3540635149_58.
   ISBN 978-3-540-69587-5.

     ^ ^a ^b Kimber, Timothy (2012). Learning definite and normal logic
   programs by induction on failure (PhD). Imperial College London. ethos
   560694. Archived from the original on 2022-10-21. Retrieved 2022-10-21.

     ^ Toth, David (2014). "Imparo is complete by inverse subsumption".
   arXiv:1407.3836 [cs.AI].

     ^ ^a ^b Cropper, Andrew; Dumančić, Sebastijan (2022-06-15).
   "Inductive Logic Programming At 30: A New Introduction". Journal of
   Artificial Intelligence Research. 74: 795. arXiv:2008.07912.
   doi:10.1613/jair.1.13507. ISSN 1076-9757.

     ^ Heindorf, Stefan; Blübaum, Lukas; Düsterhus, Nick; Werner, Till;
   Golani, Varun Nandkumar; Demir, Caglar; Ngonga Ngomo, Axel-Cyrille
   (2022). EvoLearner: Learning Description Logics with Evolutionary
   Algorithms. WWW.

     ^ Muggleton, Stephen; Santos, Jose; Tamaddoni-Nezhad, Alireza (2009).
   "ProGolem: a system based on relative minimal generalization".
   International Conference on Inductive Logic Programming. Springer.
   pp. 131–148. CiteSeerX 10.1.1.297.7992.
   doi:10.1007/978-3-642-13840-9_13. ISBN 978-3-642-13840-9.

     ^ Santos, Jose; Nassif, Houssam; Page, David; Muggleton, Stephen;
   Sternberg, Mike (2012). "Automated identification of features of
   protein-ligand interactions using Inductive Logic Programming: a hexose
   binding case study". BMC Bioinformatics. 13: 162.
   doi:10.1186/1471-2105-13-162. PMC 3458898. PMID 22783946.

     ^ De Raedt, Luc; Kersting, Kristian (2008), Probabilistic Inductive
   Logic Programming, Berlin, Heidelberg: Springer Berlin Heidelberg,
   pp. 1–27, doi:10.1007/978-3-540-78652-8_1, ISBN 978-3-540-78651-1,
   retrieved 2023-12-09

     ^ ^a ^b ^c ^d ^e ^f ^g ^h ^i Riguzzi, Fabrizio; Bellodi, Elena; Zese,
   Riccardo (2014-09-18). "A History of Probabilistic Inductive Logic
   Programming". Frontiers in Robotics and AI. 1.
   doi:10.3389/frobt.2014.00006. ISSN 2296-9144.

     ^ Koller, Daphne; Pfeffer, Avi (August 1997). Learning probabilities
   for noisy first-order rules (PDF). IJCAI.

     ^ De Raedt, L.; Kersting, K.; Kimmig, A.; Revoredo, K.; Toivonen, H.
   (March 2008). "Compressing probabilistic Prolog programs". Machine
   Learning. 70 (2–3): 151–168. doi:10.1007/s10994-007-5030-x.
   ISSN 0885-6125.

     ^ Blockeel, Hendrik; Meert, Wannes (2007), "Towards Learning
   Non-recursive LPADs by Transforming Them into Bayesian Networks",
   Inductive Logic Programming, Lecture Notes in Computer Science,
   vol. 4455, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 94–108,
   doi:10.1007/978-3-540-73847-3_16, ISBN 978-3-540-73846-6, retrieved
   2023-12-09

     ^ De Raedt, Luc; Thon, Ingo (2011), Frasconi, Paolo; Lisi, Francesca
   A. (eds.), "Probabilistic Rule Learning", Inductive Logic Programming,
   vol. 6489, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 47–58,
   doi:10.1007/978-3-642-21295-6_9, ISBN 978-3-642-21294-9,
   S2CID 11727522, retrieved 2023-12-09

     ^ Bellodi, Elena; Riguzzi, Fabrizio (2012), "Learning the Structure
   of Probabilistic Logic Programs", Inductive Logic Programming, Berlin,
   Heidelberg: Springer Berlin Heidelberg, pp. 61–75,
   doi:10.1007/978-3-642-31951-8_10, ISBN 978-3-642-31950-1, retrieved
   2023-12-09

     ^ Bellodi, Elena; Riguzzi, Fabrizio (2014-01-15). "Structure learning
   of probabilistic logic programs by searching the clause space". Theory
   and Practice of Logic Programming. 15 (2): 169–212. arXiv:1309.2080.
   doi:10.1017/s1471068413000689. ISSN 1471-0684. S2CID 17669522.

   [12px-Definition_of_Free_Cultural_Works_logo_notext.svg.png]  This
   article incorporates text from a free content work. Licensed under
   CC-BY 4.0 (license statement/permission). Text taken from A History of
   Probabilistic Inductive Logic Programming​, Fabrizio Riguzzi, Elena
   Bellodi and Riccardo Zese, Frontiers Media.

Further reading

   [edit]

     *

   Muggleton, S.; De Raedt, L. (1994). "Inductive Logic Programming:
   Theory and methods". The Journal of Logic Programming. 19–20: 629–679.
   doi:10.1016/0743-1066(94)90035-3.

     Lavrac, N.; Dzeroski, S. (1994). Inductive Logic Programming:
   Techniques and Applications. New York: Ellis Horwood.
   ISBN 978-0-13-457870-5. Archived from the original on 2004-09-06.
   Retrieved 2004-09-22.

     Visual example of inducing the grandparenthood relation by the Atom
   system.
   http://john-ahlgren.blogspot.com/2014/03/inductive-reasoning-visualized
   .html Archived 2014-03-26 at the Wayback Machine

     * v
     * t
     * e

   Programming paradigms (Comparison by language)

   Imperative

   Structured
     * Jackson structures
     * Block-structured
     * Modular
     * Non-structured
     * Procedural
     * Programming in the large and in the small
     * Design by contract
     * Invariant-based
     * Nested function

   Object-oriented
   (comparison, list)
     * Class-based, Prototype-based, Object-based
     * Agent
     * Immutable object
     * Persistent
     * Uniform Function Call Syntax

   Declarative

   Functional
   (comparison)
     * Recursive
     * Anonymous function (Partial application)
     * Higher-order
     * Purely functional
     * Total
     * Strict
     * GADTs
     * Dependent types
     * Functional logic
     * Point-free style
     * Expression-oriented
     * Applicative, Concatenative
     * Function-level, Value-level

   Dataflow
     * Flow-based
     * Reactive (Functional reactive)
     * Signals
     * Streams
     * Synchronous

   Logic
     * Abductive logic
     * Answer set
     * Constraint (Constraint logic)
     * Inductive logic
     * Nondeterministic
     * Ontology
     * Probabilistic logic
     * Query

   DSL
     * Algebraic modeling
     * Array
     * Automata-based (Action)
     * Command (Spacecraft)
     * Differentiable
     * End-user
     * Grammar-oriented
     * Interface description
     * Language-oriented
     * List comprehension
     * Low-code
     * Modeling
     * Natural language
     * Non-English-based
     * Page description
     * Pipes and filters
     * Probabilistic
     * Quantum
     * Scientific
     * Scripting
     * Set-theoretic
     * Simulation
     * Stack-based
     * System
     * Tactile
     * Templating
     * Transformation (Graph rewriting, Production, Pattern)
     * Visual

   Concurrent,
   distributed,
   parallel

     * Actor-based
     * Automatic mutual exclusion
     * Choreographic programming
     * Concurrent logic (Concurrent constraint logic)
     * Concurrent OO
     * Macroprogramming
     * Multitier programming
     * Organic computing
     * Parallel programming models
     * Partitioned global address space
     * Process-oriented
     * Relativistic programming
     * Service-oriented
     * Structured concurrency

   Metaprogramming

     * Attribute-oriented
     * Automatic (Inductive)
     * Dynamic
     * Extensible
     * Generic
     * Homoiconicity
     * Interactive
     * Macro (Hygienic)
     * Metalinguistic abstraction
     * Multi-stage
     * Program synthesis (Bayesian, Inferential, by demonstration, by
       example)
     * Reflective
     * Self-modifying code
     * Symbolic
     * Template

   Separation
   of concerns

     * Aspects
     * Components
     * Data-driven
     * Data-oriented
     * Event-driven
     * Features
     * Intentional
     * Literate
     * Roles
     * Subjects

   Retrieved from
   "https://en.wikipedia.org/w/index.php?title=Inductive_logic_programming
   &oldid=1256017671"

   Category:
     * Inductive logic programming

   Hidden categories:
     * Articles containing potentially dated statements from 2022
     * All articles containing potentially dated statements
     * Webarchive template wayback links
     * All articles with dead external links
     * Articles with dead external links from October 2022
     * Articles with permanently dead external links
     * Free-content attribution
     * Free content from Frontiers Media
     * CS1: long volume value

     * This page was last edited on 7 November 2024, at 20:21 (UTC).
     * Text is available under the Creative Commons Attribution-ShareAlike
       4.0 License; additional terms may apply. By using this site, you
       agree to the Terms of Use and Privacy Policy. Wikipedia® is a
       registered trademark of the Wikimedia Foundation, Inc., a
       non-profit organization.

     * Privacy policy
     * About Wikipedia
     * Disclaimers
     * Contact Wikipedia
     * Code of Conduct
     * Developers
     * Statistics
     * Cookie statement
     * Mobile view

     * Wikimedia Foundation
     * Powered by MediaWiki
