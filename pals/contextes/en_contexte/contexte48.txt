     * 4 List of implementations
     * 5 Probabilistic inductive logic programming
       (BUTTON) Toggle Probabilistic inductive logic programming
       subsection
--

Inductive logic programming

--

   Inductive logic programming (ILP) is a subfield of symbolic artificial
   intelligence which uses logic programming as a uniform representation
   for examples, background knowledge and hypotheses. The term "inductive"
--
   known background knowledge and a set of examples represented as a
   logical database of facts, an ILP system will derive a hypothesised
   logic program which entails all the positive and none of the negative
   examples.
--

   Inductive logic programming is particularly useful in bioinformatics
   and natural language processing.
--
   System in 1981:^[4]^[5] a Prolog program that inductively inferred Horn
   clause logic programs from positive and negative examples.^[1] The term
   Inductive Logic Programming was first introduced in a paper by Stephen
   Muggleton in 1990, defined as the intersection of machine learning and
   logic programming.^[1] Muggleton and Wray Buntine introduced predicate
   invention and inverse resolution in 1988.^[1]^[6]

   Several inductive logic programming systems that proved influential
   appeared in the early 1990s. FOIL, introduced by Ross Quinlan in
--
   At around the same time, the first practical applications emerged,
   particularly in bioinformatics, where by 2000 inductive logic
   programming had been successfully applied to drug design,
--
   programming inherent in the early work, these fields used inductive
   logic programming techniques from a viewpoint of relational data
   mining. The success of those initial applications and the lack of
   progress in recovering larger traditional logic programs shaped the
   focus of the field.^[13]
--

   Inductive logic programming has adopted several different learning
   settings, the most common of which are learning from entailment and
   learning from interpretations.^[16] In both cases, the input is
   provided in the form of background knowledge B, a logical theory
   (commonly in the form of clauses used in logic programming), as well as
   positive and negative examples, denoted
--
   {\textstyle E^{-}} respectively. The output is given as a hypothesis H,
   itself a logical theory that typically consists of one or more clauses.

--
   As of 2022^[update], learning from entailment is by far the most
   popular setting for inductive logic programming.^[16] In this setting,
   the positive and negative examples are given as finite sets
--
   }</annotation> </semantics> :MATH]
   {\displaystyle \models } stands for logical entailment:^[16]^[17]^[18]
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
--

   An inductive logic programming system is a program that takes as an
   input logic theories
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
--
   {\displaystyle B,E^{+},E^{-}} . A system is complete if and only if for
   any input logic theories
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
--
   these input theories can be found with its hypothesis search procedure.
   Inductive logic programming systems can be roughly divided into two
   classes, search-based and meta-interpretative systems.
--
   investigated since Plotkin's first work on formalising induction in
   clausal logic in 1970.^[1]^[20] Techniques used include least general
   generalisation, based on anti-unification, and inverse resolution,
--

   To account for background knowledge, inductive logic programming
   systems employ relative least general generalisations, which are
--
   resolution step to compute possible resolving clauses. Two types of
   inverse resolution operator are in use in inductive logic programming:
   V-operators and W-operators. A V-operator takes clauses
--
   Inverse resolution was first introduced by Stephen Muggleton and Wray
   Buntine in 1988 for use in the inductive logic programming system
   Cigol.^[6] By 1993, this spawned a surge of research into inverse
--
   Questions of completeness of a hypothesis search procedure of specific
   inductive logic programming system arise. For example, the Progol
   hypothesis search procedure based on the inverse entailment inference
--
   Rather than explicitly searching the hypothesis graph, metainterpretive
   or meta-level systems encode the inductive logic programming program as
   a meta-level logic program which is then solved to obtain an optimal
   hypothesis. Formalisms used to express the problem specification
--
   meta-interpreter in Prolog, while ASPAL and ILASP are based on an
   encoding of the inductive logic programming problem in answer set
   programming.^[30]
--

Probabilistic inductive logic programming

--

   Probabilistic inductive logic programming adapts the setting of
   inductive logic programming to learning probabilistic logic programs.
   It can be considered as a form of statistical relational learning
   within the formalism of probabilistic logic programming.^[34]^[35]

   Given
    1. background knowledge as a probabilistic logic program B, and
    2. a set of positive and negative examples
--

   the goal of probabilistic inductive logic programming is to find a
   probabilistic logic program
   [MATH: <semantics> <mrow class="MJX-TeXAtom-ORD"> <mstyle
--
   and the probability parameters of H. Just as in classical inductive
   logic programming, the examples can be given as examples or as
   (partial) interpretations.^[35]
--
   In the same year, Meert, W. et al. introduced a method for learning
   parameters and structure of ground probabilistic logic programs by
   considering the Bayesian networks equivalent to them and applying
--
   ProbFOIL, introduced by De Raedt and Ingo Thon in 2010, combined the
   inductive logic programming system FOIL with ProbLog. Logical rules are
   learned from probabilistic data in the sense that both the examples
--
   In 2011, Elena Bellodi and Fabrizio Riguzzi introduced SLIPCASE, which
   performs a beam search among probabilistic logic programs by
   iteratively refining probabilistic theories and optimizing the
--
    1. ^ ^a ^b ^c ^d ^e ^f Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de
       (1997). Foundations of inductive logic programming. Lecture notes
       in computer science Lecture notes in artificial intelligence.
--

   Lassez, J.-L.; Plotkin, G., eds. (1991). Computational logic : essays
   in honor of Alan Robinson. MIT Press. pp. 199–254.
--

     ^ Quinlan, J. R. (August 1990). "Learning logical definitions from
   relations". Machine Learning. 5 (3): 239–266. doi:10.1007/bf00117105.
--
     ^ ^a ^b ^c ^d Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997).
   Foundations of inductive logic programming. Lecture notes in computer
   science Lecture notes in artificial intelligence. Berlin Heidelberg:
--
     ^ Cropper, Andrew; Dumančić, Sebastijan; Evans, Richard; Muggleton,
   Stephen (2022). "Inductive logic programming at 30". Machine Learning.
   111 (1): 147–172. doi:10.1007/s10994-021-06089-1. ISSN 0885-6125.
--
     ^ Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997). Foundations of
   inductive logic programming. Lecture notes in computer science Lecture
   notes in artificial intelligence. Berlin Heidelberg: Springer. p. 255.
--
     ^ Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997). Foundations of
   inductive logic programming. Lecture notes in computer science Lecture
   notes in artificial intelligence. Berlin Heidelberg: Springer. p. 286.
--
     ^ ^a ^b Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997).
   Foundations of inductive logic programming. Lecture notes in computer
   science Lecture notes in artificial intelligence. Berlin Heidelberg:
--
   learning". Proceedings of the 13th international conference on
   inductive logic programming. LNCS. Vol. 2835. Springer. pp. 311–328.
   CiteSeerX 10.1.1.212.6602. doi:10.1007/978-3-540-39917-9_21.
--
   learning connected Horn theories". Proceedings of the 10th
   international conference on logic programing and nonmonotonic
   reasoning. LNCS. Vol. 575. Springer. pp. 169–181.
--

     ^ ^a ^b Kimber, Timothy (2012). Learning definite and normal logic
   programs by induction on failure (PhD). Imperial College London. ethos
--
     ^ Bellodi, Elena; Riguzzi, Fabrizio (2014-01-15). "Structure learning
   of probabilistic logic programs by searching the clause space". Theory
   and Practice of Logic Programming. 15 (2): 169–212. arXiv:1309.2080.
--
     * Dependent types
     * Functional logic
     * Point-free style
--
   Logic
     * Abductive logic
     * Answer set
     * Constraint (Constraint logic)
     * Inductive logic
     * Nondeterministic
     * Ontology
     * Probabilistic logic
     * Query
--
     * Choreographic programming
     * Concurrent logic (Concurrent constraint logic)
     * Concurrent OO
--
   Retrieved from
   "https://en.wikipedia.org/w/index.php?title=Inductive_logic_programming
   &oldid=1256017671"
--
   Category:
     * Inductive logic programming

